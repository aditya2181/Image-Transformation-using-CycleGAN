{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHwEZbNuIAku"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 1, 4, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 1, 4, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "G = Generator().to(device)\n",
        "F = Generator().to(device)\n",
        "D_X = Discriminator().to(device)\n",
        "D_Y = Discriminator().to(device)\n",
        "\n",
        "# Initialize weights\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "G.apply(init_weights)\n",
        "F.apply(init_weights)\n",
        "D_X.apply(init_weights)\n",
        "D_Y.apply(init_weights)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(list(G.parameters()) + list(F.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_X = optim.Adam(D_X.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss functions\n",
        "adversarial_loss = nn.MSELoss()\n",
        "cycle_loss = nn.L1Loss()\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform):\n",
        "        self.images = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert('RGB')\n",
        "        return self.transform(img)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "loader_X = DataLoader(ImageDataset('path/to/horse/images', transform), batch_size=4, shuffle=True)\n",
        "loader_Y = DataLoader(ImageDataset('path/to/zebra/images', transform), batch_size=4, shuffle=True)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for real_X, real_Y in zip(loader_X, loader_Y):\n",
        "        real_X, real_Y = real_X.to(device), real_Y.to(device)\n",
        "\n",
        "        # Train Generators\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_Y = G(real_X)\n",
        "        fake_X = F(real_Y)\n",
        "        loss_G = adversarial_loss(D_Y(fake_Y), torch.ones_like(D_Y(fake_Y)))\n",
        "        loss_G += adversarial_loss(D_X(fake_X), torch.ones_like(D_X(fake_X)))\n",
        "        loss_G += cycle_loss(F(fake_Y), real_X) + cycle_loss(G(fake_X), real_Y)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Train Discriminators\n",
        "        optimizer_D_X.zero_grad()\n",
        "        loss_D_X = (adversarial_loss(D_X(real_X), torch.ones_like(D_X(real_X))) +\n",
        "                    adversarial_loss(D_X(fake_X.detach()), torch.zeros_like(D_X(fake_X.detach())))) / 2\n",
        "        loss_D_X.backward()\n",
        "        optimizer_D_X.step()\n",
        "\n",
        "        optimizer_D_Y.zero_grad()\n",
        "        loss_D_Y = (adversarial_loss(D_Y(real_Y), torch.ones_like(D_Y(real_Y))) +\n",
        "                    adversarial_loss(D_Y(fake_Y.detach()), torch.zeros_like(D_Y(fake_Y.detach())))) / 2\n",
        "        loss_D_Y.backward()\n",
        "        optimizer_D_Y.step()\n",
        "\n",
        "torch.save(G.state_dict(), 'G.pth')\n",
        "torch.save(F.state_dict(), 'F.pth')\n",
        "torch.save(D_X.state_dict(), 'D_X.pth')\n",
        "torch.save(D_Y.state_dict(), 'D_Y.pth')\n",
        "\n",
        "def show_images(real, fake):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(real.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
        "    axes[0].set_title(\"Real Image\")\n",
        "    axes[1].imshow(fake.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
        "    axes[1].set_title(\"Fake Image\")\n",
        "    plt.show()\n",
        "\n",
        "real_image = next(iter(loader_X)).to(device)\n",
        "fake_image = G(real_image)\n",
        "show_images(real_image[0], fake_image[0])\n",
        "\n",
        "params = {\n",
        "    'learning_rate': [0.0001, 0.0002, 0.0005],\n",
        "    'batch_size': [4, 8, 16],\n",
        "    'lambda_cycle': [5.0, 10.0, 20.0],\n",
        "    'beta1': [0.5, 0.9],\n",
        "}\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_params = None\n",
        "\n",
        "for lr, batch, lam, beta in product(params['learning_rate'],\n",
        "                                    params['batch_size'],\n",
        "                                    params['lambda_cycle'],\n",
        "                                    params['beta1']):\n",
        "    # Initialize models and optimizers with the current set of parameters\n",
        "    G = Generator().to(device)\n",
        "    F = Generator().to(device)\n",
        "    D_X = Discriminator().to(device)\n",
        "    D_Y = Discriminator().to(device)\n",
        "\n",
        "    optimizer_G = optim.Adam(list(G.parameters()) + list(F.parameters()), lr=lr, betas=(beta, 0.999))\n",
        "    optimizer_D_X = optim.Adam(D_X.parameters(), lr=lr, betas=(beta, 0.999))\n",
        "    optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=lr, betas=(beta, 0.999))\n",
        "\n",
        "    # Train for a few epochs and evaluate performance\n",
        "    loss = train_for_few_epochs(G, F, D_X, D_Y, optimizer_G, optimizer_D_X, optimizer_D_Y, lam)\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_params = (lr, batch, lam, beta)\n",
        "\n",
        "print(f\"Best Parameters: Learning Rate={best_params[0]}, Batch Size={best_params[1]}, \"\n",
        "      f\"Lambda Cycle={best_params[2]}, Beta1={best_params[3]}\")\n",
        "\n",
        "import wandb  # Weights & Biases for logging\n",
        "\n",
        "wandb.init(project=\"CycleGAN\", config=params)\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, filename=\"checkpoint.pth\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, filename)\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss_G, total_loss_D = 0, 0\n",
        "\n",
        "    for real_X, real_Y in zip(loader_X, loader_Y):\n",
        "        real_X, real_Y = real_X.to(device), real_Y.to(device)\n",
        "\n",
        "        # Train Generators\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_Y = G(real_X)\n",
        "        fake_X = F(real_Y)\n",
        "\n",
        "        loss_G = (adversarial_loss(D_Y(fake_Y), torch.ones_like(D_Y(fake_Y))) +\n",
        "                  adversarial_loss(D_X(fake_X), torch.ones_like(D_X(fake_X))) +\n",
        "                  cycle_loss(F(fake_Y), real_X) * params['lambda_cycle'] +\n",
        "                  cycle_loss(G(fake_X), real_Y) * params['lambda_cycle'])\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Train Discriminators\n",
        "        optimizer_D_X.zero_grad()\n",
        "        loss_D_X = (adversarial_loss(D_X(real_X), torch.ones_like(D_X(real_X))) +\n",
        "                    adversarial_loss(D_X(fake_X.detach()), torch.zeros_like(D_X(fake_X)))) / 2\n",
        "        loss_D_X.backward()\n",
        "        optimizer_D_X.step()\n",
        "\n",
        "        optimizer_D_Y.zero_grad()\n",
        "        loss_D_Y = (adversarial_loss(D_Y(real_Y), torch.ones_like(D_Y(real_Y))) +\n",
        "                    adversarial_loss(D_Y(fake_Y.detach()), torch.zeros_like(D_Y(fake_Y)))) / 2\n",
        "        loss_D_Y.backward()\n",
        "        optimizer_D_Y.step()\n",
        "\n",
        "        total_loss_G += loss_G.item()\n",
        "        total_loss_D += (loss_D_X.item() + loss_D_Y.item())\n",
        "\n",
        "    # Log losses to Weights & Biases\n",
        "    wandb.log({\"Generator Loss\": total_loss_G / len(loader_X),\n",
        "               \"Discriminator Loss\": total_loss_D / len(loader_X)})\n",
        "\n",
        "    # Save checkpoint every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        save_checkpoint(epoch, G, optimizer_G, filename=f\"G_epoch_{epoch}.pth\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss G: {total_loss_G:.4f}, Loss D: {total_loss_D:.4f}\")\n",
        "\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "def compute_fid(real_path, fake_path):\n",
        "    fid_value = fid_score.calculate_fid_given_paths([real_path, fake_path],\n",
        "                                                    batch_size=50,\n",
        "                                                    device=device,\n",
        "                                                    dims=2048)\n",
        "    print(f\"FID Score: {fid_value}\")\n",
        "\n",
        "# Example: After generating images, compute FID\n",
        "compute_fid('path/to/real/images', 'path/to/generated/images')\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "def show_grid(real_images, fake_images, title):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title)\n",
        "    ax.imshow(vutils.make_grid(torch.cat((real_images, fake_images)), padding=2, normalize=True).permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "real_batch = next(iter(loader_X)).to(device)\n",
        "fake_batch = G(real_batch)\n",
        "show_grid(real_batch, fake_batch, \"Real vs Generated Images\")"
      ]
    }
  ]
}